\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}
\usepackage[a4paper, margin=3cm]{geometry}
\usepackage[parfill]{parskip}			% New line instead of indent for sections
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
% \usepackage[outdir=./figs]{epstopdf}

\begin{document}
\subsection*{Notes on running the code:}
The code runs with Python 3.8.13, the packages required to run can be installed from the \\ \url{requirements.txt} file by running \url{pip} \url{install} \url{-r} \url{requirements.txt}. Running \url{python} \url{sgd.py} will train classifiers using a number of different step sizes (described in more detail below) and save figures to the directory \url{./figs} and \url{.gif} animations of the decision boundary to the main directory. To speed up the training the viz variable on line 191 can be set to \url{False} to prevent the visualization of the decision boundary.
    \section*{1. Gradient}
    The gradient of
    \begin{align}
    \begin{split}
        F_i(\theta) &= \log(1 + e^{-y_i (\langle x_i, w \rangle + b)}) \\
        &= \log(1 + e^{-y_i \langle \tilde{x}_i, \theta \rangle}) , \\ 
        \text{where} \quad \tilde{x} &= \begin{bmatrix}
            x_1, x_2, 1
        \end{bmatrix}^\top, \quad \theta = \begin{bmatrix}
            w_1, w_2, b
        \end{bmatrix}^\top
    \end{split}
\end{align}
is analytically computed as
\begin{equation}
    \nabla F_i(\theta) = \frac{-y_i \tilde{x}_i e^{-y_i\langle \tilde{x}_i, \theta \rangle}}{1 + e^{-y_i \langle \tilde{x}_i, \theta \rangle}}.
\end{equation}

This is the same as the autograd\footnote{\url{https://github.com/HIPS/autograd}} automatic differentiation of the funciton. The difference between the analytical and differentiated gradients are computed when running the \url{sgd.py} file.

\section*{2 \& 3. SGD with different step sizes}
% \section*{2 & 3. SGD with different step sizes}
% The training was done using step sizes, $\alpha = \begin{bmatrix}
%     0.001, 0.01, 0.05, 0.1, 1.0
% \end{bmatrix}$, as well as with $\alpha$ decreasing every 10th epoch to 0.9 of its previous value. The training and test errors along with the difference between training error at epoch $k$ and $k-1$ for the different step sizes are shown below. The training terminates after 250 epochs.
The training was done using step sizes, $\alpha = \begin{bmatrix}
    0.001, 0.01, 0.05, 0.1, 1.0
\end{bmatrix}$, as well as with $\alpha$ decreasing according to $\alpha_k = \frac{\alpha}{\sqrt{k+1}}$ with initial value of 0.05. The training and test errors along with the difference between training error at epoch $k$ and $k-1$ for the different step sizes are shown below. The training terminates after 250 epochs.

As is expected training with a larger step size quickly converges to a decision boundary around which it oscillates, the bigger the step size the bigger the oscillations. Among the evaluated step sizes $\alpha = 0.001$ is the only one for which the training does not converge in 250 epochs. Looking at Figure \ref{fig:boundary}, showing the final decision boundaries, it is clear that basically the same boundary is found for all $\alpha$'s except 1.0 and 0.001. For the large $\alpha$ this is due to large oscillations and for the small $\alpha$ this is due to lack of convergence. Animations of how the boundary changes throughout training can be seen in the attached \url{.gif} files.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/errors-1.000.eps}
        \caption{}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/deltas-1.000.eps}
        \caption{}
    \end{subfigure}
    \caption{}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/errors-0.100.eps}
        \caption{}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/deltas-0.100.eps}
        \caption{}
    \end{subfigure}
    \caption{}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/errors-0.050.eps}
        \caption{}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/deltas-0.050.eps}
        \caption{}
    \end{subfigure}
    \caption{}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/errors-0.010.eps}
        \caption{}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/deltas-0.010.eps}
        \caption{}
    \end{subfigure}
    \caption{}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/errors-0.001.eps}
        \caption{}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/deltas-0.001.eps}
        \caption{}
    \end{subfigure}
    \caption{}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/errors-dec.eps}
        \caption{}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/deltas-dec.eps}
        \caption{}
    \end{subfigure}
    \caption{}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/boundary-1.000.eps}
        \caption{}
    \end{subfigure}
    ~
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/boundary-0.100.eps}
        \caption{}
    \end{subfigure}
    
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/boundary-0.050.eps}
        \caption{}
    \end{subfigure}
    ~
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/boundary-0.010.eps}
        \caption{}
    \end{subfigure}
   
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/boundary-0.001.eps}
        \caption{}
    \end{subfigure}
    ~
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/boundary-dec.eps}
        \caption{Decreasing $\alpha$}
    \end{subfigure}
    \caption{}
    \label{fig:boundary}
\end{figure}

\section*{4. Final remarks}
Notable for the experiments presented here is that the test error in general is slightly lower than the training error. As the two classes clealy are not linearly separable the classifier will not fit to noisy edge cases in the training data to a great extent. Hence, the smaller test error is probably explained by a simpler dataset, i.e. less data points in the misclassified regions, due to the stochastic data generation.

I ran the training for a set number of epochs, mainly for the purpose of visualisation and comparison between the different learning rates. In this setup it obviously does not make any difference to keep training after the training error has converged. The training could be stopped when the difference $\text{error}_{k-1} - \text{error}_k$ is small enough or negative which would suggest that the gradient step passed the minima.

\end{document}